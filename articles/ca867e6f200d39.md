---
title: "AWS Bedrockã§å›ç­”ã®ç¶šãã‚’å–å¾—ã™ã‚‹æ–¹æ³•"
emoji: "ğŸ’­"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["aws", "bedrock", "python", "anthropic"]
published: true
---

## æ–¹æ³•

- `invoke_model`ã‚’ä½¿ã„ã¾ã™ã€‚
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒ `"stop_reason": "max_tokens"` ã®ã¨ãã€`messages` ã« `{"role": "assistant", "content": [{"type": "text", "text": {ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ}}]`ã‚’è¿½åŠ ã—ã¦ã€å†åº¦`invoke_model`ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒ `"stop_reason": "end_turn"` ã«ãªã‚‹ã¾ã§ç¶šã‘ã¾ã™ã€‚

## ç’°å¢ƒ

- Python 3
- boto3
- ãƒ¢ãƒ‡ãƒ«: Claude 3 Haiku

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

`max_tokens` ã§æ‰“ã¡åˆ‡ã‚‰ã‚Œã‚‹ãŸã³ã«å¿œç­”ã‚’å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’é€£çµã—ã¦ã„ã¾ã™ã€‚`stop_reason` ãŒ `"end_turn"` ã«ãªã‚‹ã¾ã§ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ä»•çµ„ã¿ã§ã™ã€‚

### InvokeModel API

```python
import json

import boto3

client = boto3.client("bedrock-runtime")
model_id = "anthropic.claude-3-haiku-20240307-v1:0"


def main() -> str:
    full_text = ""
    body_params = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 10,   # æ„å›³çš„ã«å°ã•ã„å€¤ã«ã—ã¦ã„ã¾ã™
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "ã‚«ã‚¨ãƒ«ã«ã¤ã„ã¦ã®ä¿³å¥ã‚’ä½œã£ã¦ãã ã•ã„"},
                ],
            },
        ],
    }

    while True:
        params = {
            "modelId": model_id,
            "body": json.dumps(body_params),
            "accept": "application/json",
            "contentType": "application/json",
        }

        print("Invoking model...")
        response = client.invoke_model(**params)
        response_body = json.loads(response.get("body").read())

        for content in response_body.get("content", []):
            full_text += content["text"].strip()

        stop_reason = response_body["stop_reason"]
        print("Stop reason: ", stop_reason)
        if stop_reason != "max_tokens":
            break

        body_params["messages"].append(
            {
                "role": "assistant",
                "content": [{"type": "text", "text": full_text}],
            },
        )

    return full_text


if __name__ == "__main__":
    generated_text = main()

    print("--------------------------")
    print(generated_text)
```

#### å®Ÿè¡Œçµæœä¾‹

```txt
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  max_tokens
Invoking model...
Stop reason:  end_turn
--------------------------
ã¯ã„ã€ã‚«ã‚¨ãƒ«ã«ã¤ã„ã¦ã®ä¿³å¥ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸã€‚

æ± ã®ã»ã¨ã‚Šã§
é™ã‹ã«é³´ãå°ã•ãªå£°å¤•æš®ã‚Œã®å½±è‡ªç„¶ã®ä¸­ã§ã®ã‚«ã‚¨ãƒ«ã€‚é™ã‹ã«éŸ¿ãå£°ã€ãã—ã¦å¤•æš®ã‚Œã®é™ã‘ã•ã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚
```

### Converse API

```py
import boto3

client = boto3.client("bedrock-runtime")
model_id = "anthropic.claude-3-haiku-20240307-v1:0"


def main(
    user_prompts: list[str],
    assistant_propmts: list[str] | None = None,
    system: str | None = None,
) -> str:
    if assistant_propmts is None:
        # åˆæœŸåŒ–
        assistant_propmts = []

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    user_message = {
        "role": "user",
        "content": [{"text": user_prompt} for user_prompt in user_prompts],
    }
    messages = [user_message]

    if assistant_propmts:
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        assistant_message = {
            "role": "assistant",
            "content": [{"text": assistant_propmt} for assistant_propmt in assistant_propmts],
        }
        messages.append(assistant_message)

    params = {
        "messages": messages,
        "modelId": model_id,
        "inferenceConfig": {
            "maxTokens": 10,  # æ„å›³çš„ã«å°ã•ã„å€¤ã«ã—ã¦ã„ã¾ã™
            "temperature": 0,
        },
    }

    if system:
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        params["system"] = [{"text": system}]

    # å®Ÿè¡Œ
    print("conversing...")
    response = client.converse(**params)

    # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ
    response_text = (
        response.get("output", {}).get("message", {}).get("content", [{}])[0].get("text")
    )
    if response_text is None:
        msg = "No response from Bedrock"
        raise Exception(msg)  # noqa: TRY002

    # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’ assistant_propmts ã«è¿½åŠ ã™ã‚‹
    assistant_propmts.append(response_text)

    stop_reason = response["stopReason"]
    print("Stop reason:", stop_reason)
    if stop_reason == "max_tokens":
        # ä¸å®Œå…¨ãªç”Ÿæˆã®å ´åˆã¯ã€ç¶šãã‚’ç”Ÿæˆã™ã‚‹
        return main(user_prompts, assistant_propmts)

    # ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆãŒå®Œäº†ã—ãŸã®ã§ã€ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆç¾¤ã‚’æ–‡å­—åˆ—ã«ã—ã¦æˆ»ã™
    return "".join(assistant_propmts)


if __name__ == "__main__":
    generated_text = main(user_prompts="ã‚«ã‚¨ãƒ«ã«ã¤ã„ã¦ã®ä¿³å¥ã‚’ä½œã£ã¦ãã ã•ã„")
    print(generated_text)
```

#### å®Ÿè¡Œçµæœä¾‹

```txt
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: max_tokens
conversing...
Stop reason: end_turn
ã¯ã„ã€ã‚«ã‚¨ãƒ«ã«ã¤ã„ã¦ã®ä¿³å¥ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸã€‚

æ± ã®æ°´é¢ã«
è·³ã­ã‚‹å°ã•ãªå½±
å¤ã®å¤•æš®ã‚Œ

ç·‘ã®è‘‰é™°ã§
é³´ãå£°éŸ¿ãæ¸¡ã‚‹
æœˆå¤œã®ã‚« ã‚¨ãƒ«

è›™ã®å£°ã«
å¿ƒå’Œã‚€é™å¯‚
åˆå¤ã®å¤œ

ã„ã‹ãŒã§ ã—ã‚‡ã†ã‹ã€‚ã‚«ã‚¨ãƒ«ã®å§¿ã‚„é³´ãå£°ã€å­£ç¯€æ„Ÿã‚’è¡¨ç¾ã—ã¦ã¿ã¾ã—ãŸã€‚ä¿³å¥ã®çŸ­ã„å½¢å¼ã®ä¸­ã«ã€ã‚«ã‚¨ãƒ«ã®é­…åŠ›ã‚’æ„Ÿã˜å–ã£ã¦ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚
```
